{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavinbajaj/bhavin15/blob/main/SLT_10_semisupervised_notebbok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuhSs96n0v4_",
        "outputId": "2358f6fd-266e-4e17-8ae1-36c9d7dbcd9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-cv\n",
            "  Downloading keras_cv-0.6.4-py3-none-any.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.1/803.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-cv) (23.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-cv) (2023.6.3)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras-cv) (4.9.2)\n",
            "Collecting keras-core (from keras-cv)\n",
            "  Downloading keras_core-0.1.6-py3-none-any.whl (944 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m944.9/944.9 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.17.3)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (13.5.2)\n",
            "Collecting namex (from keras-core->keras-cv)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (0.1.8)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (8.1.7)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.4.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (5.9.5)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (4.66.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras-cv) (6.0.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras-cv) (3.16.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-cv) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-cv) (2.16.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv) (1.60.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-cv) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
            "Installing collected packages: namex, tensorflow-estimator, keras, keras-core, google-auth-oauthlib, tensorboard, tensorflow, keras-cv\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-1.0.0 keras-2.13.1 keras-core-0.1.6 keras-cv-0.6.4 namex-0.0.7 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0\n"
          ]
        }
      ],
      "source": [
        "pip install keras-cv tensorflow --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHgGwieG2Loa",
        "outputId": "a1c2da54-0b45-463a-8920-e7064fb882f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_core in /usr/local/lib/python3.10/dist-packages (0.1.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_core) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras_core) (1.23.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras_core) (13.5.2)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras_core) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras_core) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras_core) (0.1.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_core) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras_core) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras_core) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install keras_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq3Xi57q1LPL",
        "outputId": "e7defdb5-0d39-4839-ee00-9a78d7a18d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import keras_cv\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK-wgDJT2mWE"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 96\n",
        "IMAGE_CHANNELS = 3\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Algorithm hyperparameter\n",
        "UNLABELED_BATCH_SIZE = 1024\n",
        "LABELED_BATCH_SIZE = 128\n",
        "TEST_BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjXuxxjACuvf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Kp_BtLS2rNy",
        "outputId": "5be370d0-1d1d-4b78-f1b2-d96fd2b50eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANvZ2ZhI4KIR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PATH = '/content/drive/MyDrive/semi_super_tutorial/' # You can change the localisation of your mini-project about semi-supervised learning\n",
        "stl10_path = os.path.join(PATH, \"STL10\")\n",
        "os.makedirs(stl10_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlwiDVkQ4SXg",
        "outputId": "bea2b8c4-b584-4f33-b551-182e53a4003a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import STL10\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "train_labeled_dataset = STL10(root=stl10_path, split=\"train\", download=True, transform=ToTensor())\n",
        "train_unlabeled_dataset = STL10(root=stl10_path, split=\"unlabeled\", download=True, transform=ToTensor())\n",
        "test_labeled_dataset = STL10(root=stl10_path, split=\"test\", download=True, transform=ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtTyF8zr55_C"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(\n",
        "    train_labeled_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        ")\n",
        "unlabeled_loader = DataLoader(\n",
        "    train_unlabeled_dataset,\n",
        "    batch_size=UNLABELED_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    drop_last=False,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_labeled_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    drop_last=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2tWIJWDCwcx",
        "outputId": "92b48518-219e-4a67-a27e-4283d7b26dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train loss = 2.067 (acc: 0.286), validation loss = 2.115 (acc: 0.285), time 609.1s\n",
            "Epoch 2: train loss = 1.442 (acc: 0.472), validation loss = 1.634 (acc: 0.425), time 596.6s\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import time\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class SupervisedModel():\n",
        "    def __init__(self, encoder):\n",
        "        self.model = encoder\n",
        "\n",
        "    @staticmethod\n",
        "    def count_correct(\n",
        "        y_pred: torch.Tensor,\n",
        "        y_true: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns the number of correct predictions\n",
        "        \"\"\"\n",
        "        preds = torch.argmax(y_pred, dim=1)\n",
        "        return (preds == y_true).float().sum()\n",
        "\n",
        "\n",
        "    def validate_model(\n",
        "        self,\n",
        "        loss_fn,\n",
        "        dataloader: DataLoader\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Validates given model with given loss function on given DataLoader\n",
        "        \"\"\"\n",
        "        loss = 0\n",
        "        correct = 0\n",
        "        all = 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in dataloader:\n",
        "                device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "                X_batch = X_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "                y_pred = self.model(X_batch)\n",
        "                all += len(y_pred)\n",
        "                loss += loss_fn(y_pred, y_batch)\n",
        "                correct += SupervisedModel.count_correct(y_pred, y_batch)\n",
        "        return loss / all, correct / all\n",
        "\n",
        "\n",
        "    def train_model(\n",
        "        self,\n",
        "        optimiser: optim.Optimizer,\n",
        "        loss_fn,\n",
        "        train_loader: DataLoader,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Trains given model with given loss function on given DataLoader\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = self.model(X_batch)\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        loss_fn,\n",
        "        train_loader: DataLoader,\n",
        "        test_loader: DataLoader,\n",
        "        epochs=2\n",
        "    ) -> dict:\n",
        "        \"\"\"\n",
        "        Trains model and validates on training and validation data\n",
        "        \"\"\"\n",
        "        results = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [],\n",
        "                   \"val_acc\": []}\n",
        "        optimiser = optim.AdamW(self.model.parameters())\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start = time.time()\n",
        "\n",
        "            self.train_model(optimiser, loss_fn, train_loader)\n",
        "            train_loss, train_acc = self.validate_model(loss_fn, train_loader)\n",
        "            val_loss, val_acc = self.validate_model(loss_fn, test_loader)\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}: train loss = {train_loss:.3f} \"\n",
        "             f\"(acc: {train_acc:.3f}), validation loss = {val_loss:.3f} \"\n",
        "             f\"(acc: {val_acc:.3f}), time {time.time() - start:.1f}s\")\n",
        "            results[\"train_loss\"].append(float(train_loss))\n",
        "            results[\"train_acc\"].append(float(train_acc))\n",
        "            results[\"val_loss\"].append(float(val_loss))\n",
        "            results[\"val_acc\"].append(float(val_acc))\n",
        "\n",
        "        return results\n",
        "\n",
        "def evaluate_baseline():\n",
        "    encoder = torchvision.models.resnet18()\n",
        "    encoder.fc = nn.Linear(512, 10) # Resnet18 is created for 1000 classses so we need to change the last layer\n",
        "    baseline_model = SupervisedModel(encoder)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
        "    return baseline_model.fit(loss_fn, train_loader, test_loader)\n",
        "\n",
        "history_baseline = evaluate_baseline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYTDBlFWXomW",
        "outputId": "adeda7fb-dede-4df9-b710-3b08e7d725f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1\n",
            "Epoch 1: train loss = 0.017 (acc: 0.299), validation loss = 0.017 (acc: 0.299), time 536.0s\n",
            "Iteration 2\n",
            "Epoch 1: train loss = 0.023 (acc: 0.221), validation loss = 0.023 (acc: 0.221), time 531.9s\n"
          ]
        }
      ],
      "source": [
        "encoder = torchvision.models.resnet18()\n",
        "encoder.fc = nn.Linear(encoder.fc.in_features, 10)\n",
        "model = SupervisedModel(encoder)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "threshold = 0.7  # Confidence threshold for pseudo-labeling\n",
        "epochs_per_iteration = 1  # Number of epochs to train in each iteration\n",
        "\n",
        "# Train the model with pseudo-labeling\n",
        "for iteration in range(5):  # You can adjust the number of iterations\n",
        "    print(f\"Iteration {iteration + 1}\")\n",
        "\n",
        "    # Train the model on the labeled dataset\n",
        "    model.fit(loss_fn, train_loader, train_loader, epochs=epochs_per_iteration)\n",
        "\n",
        "    predictions = []\n",
        "    confidences = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, _ in unlabeled_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_pred = model.model(X_batch)\n",
        "            confidence, preds = torch.max(y_pred, dim=1)\n",
        "            predictions.extend(preds.tolist())\n",
        "            confidences.extend(confidence.tolist())\n",
        "\n",
        "    # Filter predictions above the threshold\n",
        "    pseudo_labeled_data = []\n",
        "    for i, confidence in enumerate(confidences):\n",
        "        if confidence > threshold:\n",
        "            img_tensor = torch.from_numpy(train_unlabeled_dataset.data[i])  # Convert NumPy array to tensor\n",
        "            pseudo_labeled_data.append((img_tensor, predictions[i]))\n",
        "\n",
        "    if len(pseudo_labeled_data) == 0:\n",
        "        print(\"No more predictions above the threshold. Stopping iterations.\")\n",
        "        break\n",
        "\n",
        "    # Add pseudo-labeled data to the labeled dataset\n",
        "    pseudo_labeled_dataset = torch.utils.data.TensorDataset(\n",
        "        torch.stack([data[0] for data in pseudo_labeled_data]),\n",
        "        torch.tensor([data[1] for data in pseudo_labeled_data])\n",
        "    )\n",
        "    train_labeled_dataset += pseudo_labeled_dataset\n",
        "\n",
        "# Evaluate the final model on the test set\n",
        "final_accuracy = model.validate_model(loss_fn, test_loader)\n",
        "print(f\"Final accuracy on the test set: {final_accuracy:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObThIeW/5KZ9iudKX856AD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}